version: '3.8'

services:
  # llama.cpp inference server for token generation
  llama-cpp-server:
    image: ghcr.io/ggerganov/llama.cpp:server-cuda
    container_name: orpheus-inference-server
    ports:
      - "5006:8000"
    volumes:
      - ./models:/models
      - ./outputs:/outputs
    environment:
      - CUDA_VISIBLE_DEVICES=0
    command: >
      --model /models/Orpheus-3b-Hindi-FT-Q8_0.gguf
      --ctx-size 8192
      --n-predict 8192
      --rope-scaling linear
      --n-gpu-layers 99
      --threads 8
      --parallel 4
      -ngl 99
      -cb
      --port 8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Orpheus FastAPI TTS frontend
  orpheus-fastapi:
    build:
      context: .
      dockerfile: Dockerfile.gpu
    container_name: orpheus-tts-api
    ports:
      - "5005:5005"
    volumes:
      - ./outputs:/app/outputs
      - ./static:/app/static
      - ./templates:/app/templates
    environment:
      - ORPHEUS_API_URL=http://llama-cpp-server:8000/v1/completions
      - ORPHEUS_API_TIMEOUT=120
      - ORPHEUS_MAX_TOKENS=8192
      - ORPHEUS_TEMPERATURE=0.6
      - ORPHEUS_TOP_P=0.9
      - ORPHEUS_SAMPLE_RATE=24000
      - ORPHEUS_PORT=5005
      - ORPHEUS_HOST=0.0.0.0
      - ORPHEUS_MODEL_NAME=Orpheus-3b-Hindi-FT-Q8_0.gguf
      - CUDA_VISIBLE_DEVICES=0
    depends_on:
      llama-cpp-server:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  # Model downloader service
  model-init:
    image: ghcr.io/huggingface/huggingface-hub:latest
    container_name: orpheus-model-init
    volumes:
      - ./models:/models
    environment:
      - HF_HOME=/models
    command: >
      bash -c "
      cd /models &&
      huggingface-cli download lex-au Orpheus-3b-Hindi-FT-Q8_0.gguf --local-dir . &&
      echo 'Model downloaded successfully'
      "
    profiles:
      - init

volumes:
  models:
  outputs:
