# LLM Inference Server Configuration
ORPHEUS_API_URL=http://localhost:5006/v1/completions
ORPHEUS_API_TIMEOUT=120

# Model Parameters (Orpheus-specific)
ORPHEUS_MAX_TOKENS=8192
ORPHEUS_TEMPERATURE=0.6
ORPHEUS_TOP_P=0.9
ORPHEUS_REPETITION_PENALTY=1.1

# Audio Output Configuration
ORPHEUS_SAMPLE_RATE=24000

# FastAPI Server Configuration
ORPHEUS_PORT=5005
ORPHEUS_HOST=0.0.0.0

# Model Name (use language-specific variants)
ORPHEUS_MODEL_NAME=Orpheus-3b-Hindi-FT-Q8_0.gguf

# GPU Configuration
CUDA_VISIBLE_DEVICES=0
